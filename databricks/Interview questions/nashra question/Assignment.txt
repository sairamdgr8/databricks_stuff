
---> Approach

1--> reading text file in RDD and converting to DF
2--> adding schema
3--> from pyspark.sql import *
4 --> Inner join two DFs
5-->SUM(ltr) GROUP BY tmsDate

-------------------------------------------------------------------------------------------------------------------------------------------------------------
import findspark
findspark.init()

from pyspark.sql import SparkSession
spark  = SparkSession.builder\
.master("ip")\
.appName("my_assignment")\
.getOrCreate()
sc=spark.sparkcontext

rdd = sc.textfile('/FileStore/tables/Power_Event-6.txt')
rdd1= rdd.map(lambda x:x.split('|'))
df= spark.createDataFrame(rdd1, ["asset_identifier","tmsdate","tmsInLong","type","event"], schm)
df.show()

from pyspark.sql.types import *

schm = StructType([
	StructField("asset_identifier", StringType(),True),
	StructField("tmsdate", StringType(),True),
	StructField("tmsInLong", StringType(),True),
	StructField("type", StringType(),True),
	StructField("event", StringType(),True),
])
df.write.mode(SaveMode.Overwrite).format("csv").option("header", true).option("path", "/FileStore/tables/Power_Event-6.1.csv")

-------------------------------------------------------------------------------------------------------------------------------------------------------------
import findspark
findspark.init()

from pyspark.sql import SparkSession
spark  = SparkSession.builder\
.master("ip")\
.appName("my_assignment1")\
.getOrCreate()
sc=spark.sparkcontext


rdd2 = sc.textfile('dbfs:/FileStore/tables/assigment/Fuel_Consumption.txt')
rdd3= rdd2.map(lambda x:x.split('|'))
df1= spark.createDataFrame(rdd3, ["asset_identifier","tmsdate","tmsInLong","type","ltr"], schm1)
df1.show()

from pyspark.sql.types import *

schm1 = StructType([
	StructField("asset_identifier", StringType(),True),
	StructField("tmsdate", StringType(),True),
	StructField("tmsInLong", StringType(),True),
	StructField("type", StringType(),True),
	StructField("ltr", StringType(),True),
])
df.write.mode(SaveMode.Overwrite).format("csv").option("header", true).option("path", "dbfs:/FileStore/tables/assigment/Fuel_Consumption1.csv")
------------------------------------------------------------------------------------------------------------------------------------------------------------
consumption per session
------------------------------------------------------------------------------------------------------------------------------------------------------------
first_df = spark.read.csv('dbfs:/FileStore/tables/assigment/Fuel_Consumption1.csv')
	  .option("header","True")
	  .option("dateFormat", "yyyy-MM-dd HH:mm:ss")
sec_df = spark.read.csv('/FileStore/tables/Power_Event-6.1.csv')
	  .option("header","True")
	  .option("dateFormat", "yyyy-MM-dd HH:mm:ss")
df_output = first_df.join(sec_df, on=["asset_identifier", "tmsdate"], how = "outer")
fuel_consumption= ???????????????????????


-------------------------------------------------------------------------------------------------------------------------------------------------------------
Daily report
-------------------------------------------------------------------------------------------------------------------------------------------------------------
first_df = spark.read.csv('dbfs:/FileStore/tables/assigment/Fuel_Consumption1.csv')
	  .option("header","True")
	  .option("dateFormat", "yyyy-MM-dd HH:mm:ss")
sec_df = spark.read.csv('/FileStore/tables/Power_Event-6.1.csv')
	  .option("header","True")
	  .option("dateFormat", "yyyy-MM-dd HH:mm:ss")
df_output = first_df.join(sec_df, on=["asset_identifier", "tmsdate"], how = "outer")
report_df=df_output.groupBY('tmsdate','event').sum('ltr')
report_df.show()
-------------------------------------------------------------------------------------------------------------------------------------------------------------



